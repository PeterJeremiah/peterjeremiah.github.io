<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Mingyuan Yang</title>
  
  <meta name="author" content="Jon Barron">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="images/icons8-ico-64.ico">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Mingyuan Yang/杨明远</name>
              </p>
              <p>I am an undergraduate majoring in Computer Science from Honors College, Northwestern Polytechnical University (CHN).
              </p>
              <p>At <a href="https://iopen.nwpu.edu.cn/">School of Artificial Intelligence, OPtics and ElectroNics</a>, Northwestern Polytechnical University, I am advised by <a href="https://iopen.nwpu.edu.cn/info/1251/1852.htm">Prof. Bin Zhao</a>. My research interests focus on multi-modal learning.
              </p>
              <p>I has done research on Bayesian statistics and time series analysis at <a href="https://jlucqe.jlu.edu.cn/">Center for Quantitative Economics</a>, JiLin University.
              </p>


              <p style="text-align:center">
                <a href="mailto:yangmingyuan@mail.nwpu.edu.com">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com.hk/citations?user=W_BeOckAAAAJ&hl=zh-CN">Google Scholar</a> &nbsp/&nbsp
                <!--
                <a href="data/JonBarron-CV.pdf">CV</a> &nbsp/&nbsp
                <a href="data/JonBarron-bio.txt">Bio</a> &nbsp/&nbsp
                <a href="https://twitter.com/jon_barron">Twitter</a> &nbsp/&nbsp  -->
                <a href="https://github.com/PeterJeremiah">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:30%;max-width:30%">
              <a href="images/ME.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/ME.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>



        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>News</heading>
            <p>
              <strong>[2025-03]</strong> One paper is being submitted to the ICCV conference, thanks to all co-authors!
            </p>
            <p>
              <strong>[2024-12]</strong> A software copyright and a patent on deep learning were accepted, thanks to all co-authors!
            </p>
            <p>
              <strong>[2024-11]</strong> Awarded the China National Scholarship for undergraduate student!
            </p>
            <p>
              <strong>[2024-11]</strong> Awarded the Academic Star and the Science and Technology Star!
            </p>
            <p>
              <strong>[2024-07]</strong> Attended 2024 Asian Summer School in Econometrics and Statistics!
            </p>
            <p>
              <strong>[2024-07]</strong> A patent on Bayesian algorithm is being submitted, thanks to all co-authors!
            </p>
            <p>
              <strong>[2024-05]</strong> Start scientific research on multi-modal large models at iOPEN institute of NWPU!
            </p>
            <p>
              <strong>[2024-05]</strong> Our team won the championship in Robocup China Robot Competition, thanks to all teammates!
            </p>
            <p>
              <strong>[2023-12]</strong> Start scientific research on Bayesian statistics and time series analysis at Center for Quantitative Economics of JiLin University!
            </p>
            <p>
              <strong>[2023-10]</strong> Awarded the School Scholarship for undergraduate student!
            </p>
            <p>
              <strong>[2023-06]</strong> Two of the team's projects were selected as National-level Undergraduate Training Programs for Innovation and Entrepreneurship!
            </p>
            <p>
              <strong>[2023-05]</strong> Successfully join the NWPU Dancing Robot Research & Training Base!
            </p>
            <p>
              <strong>[2022-09]</strong> Start undergraduate studies at Northwestern Polytechnical University!
            </p>

          </td>
        </tr>
      </tbody></table>

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
        <tr>
        <td style="padding:20px;width:100%;vertical-align:middle">
          <heading>Selected Honors</heading>
          <p>
            &#x2022 China National Scholarship for undergraduate student <strong>(highest student honor in China, awarded to top 0.2%) </strong>, 2024.
          </p>
        </td>
      </tr>
    </tbody></table>

<!--
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
        <tr>
        <td style="padding:20px;width:100%;vertical-align:middle">
          <heading>Services</heading>
          <p>
            <strong>Conference Reviewer:</strong> CVPR 2022-2024, ECCV 2022/2024, ICCV 2023, AAAI 2023-2025
          </p>
          <p>
            <strong>Journal Reviewer:</strong> TPAMI, TMM, TCSVT
          </p>
        </td>
      </tr>
    </tbody></table>
-->


    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
      <!--
      <tbody>
      <heading style="padding:20px">Survey</heading>
      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src="images/avlearning.jpg" alt="clean-usnob" width="320" height="160">
        </td>
        <td width="75%" valign="middle">
          <papertitle>Learning in Audio-visual Context: A Review, Analysis, and New Perspective</papertitle>
          <br>
          <br>
          <strong>Yake Wei</strong>, <a href="https://dtaoo.github.io">Di Hu</a>, <a href="https://yapengtian.org/">Yapeng Tian</a>, <a href="https://scholar.google.com/citations?user=ahUibskAAAAJ&hl=zh-CN">Xuelong Li</a>
          <br>
          <br>

          <br>
          <a href="https://arxiv.org/abs/2208.09579">arXiv</a> / <a href="https://gewu-lab.github.io/audio-visual-learning/">website</a> / <a href="https://gewu-lab.github.io/awesome-audiovisual-learning/">awesome list</a> 
          <br>
          <p>A systematical survey about the audio-visual learning field.</p>
        </td>
      </tr>

      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src="images/low_quality.png" alt="clean-usnob" width="320" height="160">
        </td>
        <td width="75%" valign="middle">
          <papertitle>Multimodal Fusion on Low-quality Data: A Comprehensive Survey</papertitle>
          <br>
          <br>
          Qingyang Zhang, <strong>Yake Wei</strong>, Zongbo Han, Huazhu Fu, Xi Peng, Cheng Deng, Qinghua Hu, Cai Xu, Jie Wen, <a href="https://dtaoo.github.io">Di Hu</a>, Changqing Zhang
          <br>
          <br>

          <br>
          <a href="https://arxiv.org/abs/2404.18947">arXiv</a> / <a href="https://github.com/QingyangZhang/awesome-low-quality-multimodal-learning">awesome list</a> 
          <br>
          <p>A systematical survey about fusion of low-quality multi-modal data.</p>
        </td>
      </tr>
    -->
    </tbody></table>

    <br>
    
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <heading style="padding:20px">Publications</heading>(&#42; equal contribution)
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/white.png" alt="clean-usnob" width="320" height="160">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Universal Blind Image Restoration via Vision-Language Model Engine</papertitle>
              <br>
              <br>
              Guanzhou Lan, Qianli Ma, <strong>Mingyuan Yang</strong>, Yixuan Lou, Zhigang Wang, Dong Wang, Xuelong Li, Bin Zhao
              <br>
              <br>
              <em>ICCV</em>, 2025
              <br>
              <a href="https://openreview.net/forum?id=g6NN4FmCzR">OpenReview</a> / <a href="">code
              </a>
              <br>
              <p>This paper proposes UBIR, a universal blind image restoration framework, validating the synergistic potential of multimodal pretrained models for complex real-world degradation scenarios.

              </p>
            </td>
          </tr>

<!---
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/bimodulation.jpg" alt="clean-usnob" width="320" height="160">
            </td>
            <td width="75%" valign="middle">
              <papertitle>On-the-fly Modulation for Balanced Multimodal Learning</papertitle>
              <br>
              <br>
              <strong>Yake Wei</strong>, <a href="https://dtaoo.github.io">Di Hu</a>, Henghui Du, Ji-Rong Wen
              <br>
              P.S. Thanks the valuable help from Zequn Yang
              <br>
              <br>
              <em>T-PAMI</em>, 2024
              <br>
              <a href="https://arxiv.org/abs/2410.11582">arXiv</a> / <a href="https://github.com/GeWu-Lab/BML_TPAMI2024">code
              </a>
              <br>
              <p>Analyze and modulate imbalanced uni-modal learning from both feed-forward and back-propagation stage.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/reinit.jpg" alt="clean-usnob" width="320" height="160">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Diagnosing and Re-learning for Balanced Multimodal Learning</papertitle>
              <br>
              <br>
              <strong>Yake Wei</strong>, Siwei Li, Ruoxuan Feng, <a href="https://dtaoo.github.io">Di Hu</a>
              <br>
              <br>
              <em>ECCV</em>, 2024
              <br>
              <a href="https://arxiv.org/abs/2407.09705">arXiv</a> / <a href="https://github.com/GeWu-Lab/Diagnosing_Relearning_ECCV2024">code
              </a>
              <br>
              <p>Dynimically re-initialize uni-modal encoder to enhance both worse-learnt and well-learnt modalities.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/mmpareto.jpg" alt="clean-usnob" width="320" height="160">
            </td>
            <td width="75%" valign="middle">
              <papertitle>MMPareto: Boosting Multimodal Learning with Innocent Unimodal Assistance</papertitle>
              <br>
              <br>
              <strong>Yake Wei</strong>, <a href="https://dtaoo.github.io">Di Hu</a>
              <br>
              <br>
              <em>ICML</em>, 2024
              <br>
              <a href="https://arxiv.org/abs/2405.17730">arXiv</a> / <a href="https://github.com/GeWu-Lab/MMPareto_ICML2024">code
              </a>
              <br>
              <p>Solve conflicts between multi-modal and uni-modal gradients under multi-modal scenarios.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/sample-level.jpg" alt="clean-usnob" width="320" height="160">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Enhancing Multimodal Cooperation via Sample-level Modality Valuation</papertitle>
              <br>
              <br>
              <strong>Yake Wei</strong>, Ruoxuan Feng, Zihe Wang, <a href="https://dtaoo.github.io">Di Hu</a>
              <br>
              <br>
              <em>CVPR</em>, 2024
              <br>
              <a href="https://arxiv.org/abs/2309.06255">arXiv</a> / <a href="https://github.com/GeWu-Lab/Valuate-and-Enhance-Multimodal-Cooperation">code
              </a>
              <br>
              <p>Observe and improve the fine-grained cooperation between modalities at sample-level.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/mmrobust.jpg" alt="clean-usnob" width="320" height="160">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Quantifying and Enhancing Multi-modal Robustness with Modality Preference</papertitle>
              <br>
              <br>
              Zequn Yang, <strong>Yake Wei</strong>, Ce Liang, <a href="https://dtaoo.github.io">Di Hu</a>
              <br>
              <br>
              <em>ICLR</em>, 2024
              <br>
              <a href="https://arxiv.org/pdf/2402.06244.pdf">arXiv</a> / <a href="https://github.com/GeWu-Lab/Certifiable-Robust-Multi-modal-Training">code
              </a>
              <br>
              <p>Analyze essential components for multi-modal robustness and delve into the
                limitations imposed by modality preference.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/geometric-mvc.jpg" alt="clean-usnob" width="320" height="160">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Geometric-inspired graph-based Incomplete Multi-view Clustering</papertitle>
              <br>
              <br>
              Zequn Yang, Han Zhang, <strong>Yake Wei</strong>, Zheng Wang, Feiping Nie, <a href="https://dtaoo.github.io">Di Hu</a>
              <br>
              <br>
              <em>Pattern Recognition</em>, 2023
              <br>
              <a href="https://www.sciencedirect.com/science/article/pii/S0031320323007793">paper</a> / <a href="https://github.com/GeWu-Lab/Geometric-Inspired-Graph-based-Incomplete-Multi-view-Clustering">code
              </a>
              <br>
              <p>Conduct geometric analyses to mitigate missing views in weight aggregation.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/balance2022.jpg" alt="clean-usnob" width="320" height="160">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Balanced Multimodal Learning via On-the-fly Gradient Modulation</papertitle>
              <br>
              <br>
              Xiaokang Peng*, <strong>Yake Wei*</strong>, <a href="https://antony0621.github.io/">Andong Deng</a>, Dong Wang, <a href="https://dtaoo.github.io">Di Hu</a>
              <br>
              <br>
              <em>CVPR</em>, 2022 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
              <br>
              <a href="https://arxiv.org/pdf/2203.15332.pdf">arXiv</a> / <a href="https://github.com/GeWu-Lab/OGM-GE_CVPR2022">code
              </a>
              <br>
              <p>Alleviate optimization imbalance in multi-modal learning via on-the-fly gradient modulation.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/avqa2022.jpg" alt="clean-usnob" width="320" height="160">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Learning to Answer Questions in Dynamic Audio-Visual Scenarios</papertitle>
              <br>
              <br>
              <a href="https://ayameyao.github.io/">Guangyao Li*</a>, <strong>Yake Wei*</strong>, <a href="https://yapengtian.org/">Yapeng Tian*</a>, <a href="https://www.cs.rochester.edu/~cxu22/">Chenliang Xu</a>, <a href="https://scholar.google.com/citations?user=tbxCHJgAAAAJ">Ji-Rong Wen</a>, <a href="https://dtaoo.github.io">Di Hu</a>
              <br>
              <br>
              <em>CVPR</em>, 2022 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
              <br>
              <a href="https://arxiv.org/pdf/2203.14072.pdf">arXiv</a> / <a href="https://gewu-lab.github.io/MUSIC-AVQA/">project page
              </a>
              <br>
              <p>Audio-Visual Question Answering and propose MUSIC-AVQA dataset.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/pami2021.jpg" alt="clean-usnob" width="320" height="160">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Class-aware Sounding Objects Localization via Audiovisual Correspondence</papertitle>
              <br>
              <br>
              <a href="https://dtaoo.github.io">Di Hu</a>, <strong>Yake Wei</strong>, <a href="https://shvdiwnkozbw.github.io/">Rui Qian</a>, <a href="https://weiyaolin.github.io/">Weiyao Lin</a>, <a href="https://scholar.google.com/citations?user=v5LctN8AAAAJ&hl">Ruihua Song</a>, <a href="https://scholar.google.com/citations?user=tbxCHJgAAAAJ">Ji-Rong Wen</a>
              <br>
              <br>
              <em>T-PAMI</em>, 2021
              <br>
              <a href="https://arxiv.org/pdf/2112.11749.pdf">arXiv</a> / <a href="https://gewu-lab.github.io/CSOL_TPAMI2021/">project page
              </a>
              <br>
              <p>Discriminative sounding objects localization.</p>
            </td>
          </tr>
        -->
        </tbody></table>




    <br>
    
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <heading style="padding:20px">Projects</heading>
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/Rescue.jpg" alt="clean-usnob" width="320" height="160">
            </td>
            <td width="75%" valign="middle">
              <papertitle>NWPU Dancing Robot Research & Training Base</papertitle>
              <br>
              <br>
              <strong>Mingyuan Yang</strong>, Yilin Han, Zixuan Zhang, Liao Feng, Yuchi Duan, Haojie Sun
              <br>
              <br>
              <em>RoboCup</em>, 2024
              <br>              
              <a href="https://github.com/team-explorer-rescue-robot">Code (Previous version)
              </a>
              <p>Using deep learning algorithms combined with multi-sensor, search for several victims placed in the disaster scene simulation site and draw site map.
              </p>
              <!---
              <p>My main task is to use deep learning algorithms combined with thermal imaging, metal detection, sound, and dynamic target detection to accurately identify life signs at the earthquake epicenter."
              </p>-->
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/HLS.png" alt="clean-usnob" width="320" height="160">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Financial forecast acceleration algorithm development (Based on FPGA chip)</papertitle>
              <br>
              <br>
              <strong>Mingyuan Yang</strong>, Mingda Yang, Xuejing Ding, Yixin Han, Jinyun Fan, Jiaqi Li, Yang Liu
              <br>
              <br>
              <a href="">Code(Only part of it is made public)
              </a>
              <p>Bayesian algorithms and finite scoring algorithms that accelerate financial market forecasting.
              </p>
            </td>
          </tr>


        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <p font-size:small;>
                <br>
                <br>
                <div style="float:left;">
                    Updated at Feb. 2025
                </div>
                <div style="float:right;">
                    Thanks <a href="https://jonbarron.info">Jon Barron</a> for this amazing template.
                </div>
                <br>
                <br>        
            </p>                           
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
